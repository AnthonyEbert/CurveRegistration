---
title: "Registration of functional data for likelihood-free inference"
author: "Anthony Ebert, Paul Wu, Kerrie Mengersen, Fabrizio Ruggeri"
date: "16 October 2018"
geometry: margin=2cm
output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    pandoc_args: [
      "-V", "classoption=twocolumn"
    ]
    keep_tex: yes
header-includes:
  - \usepackage{algorithmicx}
  - \usepackage{algpseudocode}
  - \usepackage{algorithm}
bibliography: ../Zotero_link.bib
urlcolor: blue
---

```{r setup, include=FALSE}
# knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed = TRUE)})
knitr::opts_chunk$set(echo = FALSE, autodep = TRUE, cache = TRUE, cache.rebuild = FALSE)
library(CurveRegistration)
library(dplyr)
library(ggplot2)

true_color <- "black"
#color_scale <- scale_color_manual(values = c("red", "blue", "grey"))
color_scale <- ggthemes::scale_color_wsj()
```

\setlength{\abovedisplayskip}{-15pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\newcommand{\fsf}{\mathsf{f}}
\newcommand{\gsf}{\mathsf{g}}
\newcommand{\E}{\mathsf{E}}

# Abstract

Likelihood-free inference on functional data requires sensible distance measures for these functions. We consider the problem of functional data displaying not only amplitude variation but also phase variation. We show how current methods in curve registration can be applied to approximate Bayesian computation with the help of kernel-based methods recently developed for metrics for probability measures.  

# Introduction

In statistical problems of interest, what constitutes the sample space is sometimes a matter of perspective. For instance, in spectroscopy, if we measure the spectra of a series of samples, then we have a measurement for each combination of frequency and sample. On the one hand, the sample space could comprise the output range of the machine, a subset of the real line. On the other hand, the set of all possible spectra could be the sample space, a function space. Such data are termed functional data. Functional data analysis (FDA), proceeds with this latter approach and focuses on problems where this perspective provides useful insight [@hsing2015theoretical,@ramsay2006functional]. 

A common problem when analysing functional data is misalignment, where features of the functions do not correspond on the explanatory axis. It can be difficult to compare observations or to derive meaningful statistics whenever there is misalignment. Its not always possible to align functions with simple shifts of explanatory axes, sometimes more complex warpings of axes are required. Common statistical methods assume that only the amplitude of a function is subject to noise, however it is possible to consider the time axis to be subject to noise as well, which must be accounted for. @Kneip2008 elegantly terms this as separation of amplitude and phase variation within a functional dataset. @srivastava2011registration extended this paradigm by developing an elastic distance, with mathematically convenient properties, between functions displaying amplitude and phase variation. @srivastava2011registration developed this distance by generalising the Fisher-Rao [@rao1945information,@maybank2008fisher] metric from probability density functions to absolutely continuous functions. Simulatenously, this elastic distance on functions is the restriction of one developed for planar shapes [@joshi2007novel]. 

Parameter inference where with functional data has been developed under many different names, including: self-modelling [@kneip1988convergence], functional-modelling [@ramsay2006functional]. However, these methods require that the likelihood function be evalutated. In many complex statistical models, the likelihood function is either unavailable or intractable but it is possible to draw realisations from the model conditional on parameter values. Approximate Bayesian computation (ABC) is a likelihood-free approach to parameter inference where proposed parameter values are accepted only whenever a dissimilarity between model realisations and observed data is below some threshold. 

There is work applying ABC to models with functional data. Generally speaking the data are reduced to a set of summary statistics or an "inelastic" distance is computed directly between model realisations and data. For instance, @zhuestimating use wavelet compression to transform functional data into low-dimensional summary statistics; and @toni_approximate_2009 use sum of squared errors to directly compute dissimilarity between functional model realisations and functional data. 

<!-- The citations here are not perfect since they do not deal with functional data, just the metric on probability distributions. -->

Recently, there has been work on using metrics on probability measures such as the Wasserstein distance [@bernton_inference_2017] and maximum mean discrepancy (MMD) [@gretton_kernel_2012] and the Fisher-Rao metric [@srivastava2007riemannian] to work with functional data. 

In the following section with provide some background to functional data analysis and introduce notation. In Section \@ref(sec:ABC) we do the same for ABC. In this work we extend ABC to functional data which displays both amplitude and phase variation. We demonstrate this with three examples: the first example is a toy example with a random distribution of peak shift position (Section \@ref(sec:sG)), the second example is of passenger flow in an international airport terminal (Section \@ref(sec:Airport)), and the third example is a model of hydrological modelling \@ref(sec:Hydro)). 

# Function data analysis

Functional data analysis (FDA) is the analysis of data generated by curves or functions [@hsing2015theoretical]. Special-cases of functional datasets include longitudinal data and time series. However, the field of FDA is much broader than this [@wang2016functional]. Functional random variables take values in a function space $\mathsf{f} \in \mathcal{F}$ with associated probability measure $p$ (see @delaigle2010defining). In practice, the functional random variables are observed with error on a countable subset of the domain of the function. This is what we refer to as an empirical functional random variable (EFRV). An EFRV $f$ is itself a set of pairs $f_1, f_2, \cdots$ of the form $f_i = (t^f_{i},y^f_{i})$ representing sampling location $t^f_i$ and corresponding function output $y^f_{i}$. Therefore $f$ is of the form $\{ (t^f_{1},y^f_{1}), \cdots , (t^f_{j}, y^f_{j}), \cdots, (t^f_{n}, y^f_{n}) \}$. We append the superscript $f$ to keep track of which sampling points and functional outputs refer to which EFRV. 

When analysing functional data, a statistic of interest is an estimate of the mean function $\mu(t) = \underset{\fsf \sim p}{\E}[\mathsf{f}](t)$. In the case of EFRVs with a shared set of sampling locations ($t^{f_1} = \cdots = t^{f_i} = \cdots$), this is usually estimated with $\hat{\mu}(t_j) = 1/n_i \sum_{i = 1}^{n_i} y^{f_i}_j$. In such cases, it is possible that the resulting function is not an element of $\mathcal{F}$ due to phase variation. For instance, if $\mathcal{F}$ is the set of Gaussian functions $\phi(\mu,\sigma)$ with $\mu \sim U(0, 5)$, $\sigma = 1$ then the mean function is not Gaussian.

There are many approaches in the literature which seek to provide statistical tools for functional data in the presence of phase variation. These include: dynamic time warping [@Padoy2012632; @wang1997alignment], the Frechet distance [@rote2007computing], curve registration and, elastic functions [@srivastava2011registration]. Applications of curve registration include: growth curves [@cheng2016bayesian] and surgical workflow [@Padoy2012632]. 

We standardise the domain of $\mathsf{f^*} \in \mathbb{R}^{[0, T]}$ from $[0,T]$ to $[0,1]$ by setting $\mathsf{f}(t) = \mathsf{f^*}(t \times T)$. The idea of curve registration is to align elements of $\mathcal{F}$ with warping functions $\gamma^{\mathsf{f}}: [0,1] \mapsto [0,1]$, such that elements of the set $\mathcal{G} := \{\mathsf{f} \circ \gamma^{\mathsf{f}} | \mathsf{f} \in \mathcal{F} \}$ have aligned features according to some criteria. We adopt the elastic functions approach of @srivastava2011registration, who align $\fsf$ to another functional random variable $\mathsf{g}$ using the Fisher-Rao metric:
\begin{align}
d_{\text{FR}}(\fsf,\gsf) = \int [ q^{\fsf}(t) - q^{\gsf}(t) ]^2 \text{d} t, (\#eq:FR)
\end{align} 
where $q^{\fsf}(t) = \text{sign}(\fsf^{\prime}(t)) \times \sqrt{|\fsf^{\prime}(t)|}$. Curve $\fsf$ is aligned to $\gsf$ by defining a warping function $\gamma \in \Gamma$ where $\Gamma = \{\gamma \in [0,1]^{[0, 1]} | \gamma \text{ is invertible and } \gamma(0) = 0 \}$ to minimise $d_{\text{FR}}(\fsf \circ \gamma, \gsf)$. One advantage of using $d_{\text{FR}}$ for curve registration is that:
\begin{align*} 
d_{\text{FR}}(\fsf, \gsf) = d_{\text{FR}}(\fsf \circ \gamma, \gsf \circ \gamma), 
\end{align*}
in other words, the Fisher-Rao metric is invariant to shared warpings. This implies that the discrepancy of amplitudes (amplitude distance) between $\fsf$ and $\gsf$, defined as
\begin{align*}
d_{\text{amp}}(\fsf,\gsf) := \inf_{\gamma \in \Gamma} d_{\text{FR}}(\fsf \circ \gamma, \gsf),
\end{align*}
is symmetric [@srivastava2011registration]. In practice, $\fsf$ are smoothed empirical functions of noisy data $f$ observed on a countable subset of the domain. This point is briefly discussed by @tucker2014analysis, but not other authors. The R package fdasrvf [@tuckerCRAN], which implements the approach of @srivastava2011registration, uses a cubic spline approximation to approximate the smoothed first derivative $\fsf'(t)$. 

For simplicity and without loss of generality we consider problems where the functional data comprise a single observed EFRV. We extend curve registration to the likelihood-free domain with approximate Bayesian computation (ABC). We proceed now with a brief background of ABC, for a detailed exposition on the subject see @sisson2018handbook.

# Approximate Bayesian Computation \label{sec:ABC}

Suppose we can draw samples from, but not evaluate a probability distribution $p(\cdot|\theta)$. Furthermore, we wish to draw samples $\pi(\theta|x)$ with observed data $x$ to perform inference on $\theta$. In such cases simulation-based approaches might be necessary; these are useful whenever $p(\cdot|\theta)$ represents a complicated simulator for generating realisations $x_{\theta} \sim p(\cdot|\theta)$. 

The theoretical foundation of ABC rests on the fact that if we draw samples $\theta$ independently from the prior $\pi(\theta)$ and use each $\theta$ to generate a corresponding $x_{\theta}$ from $p(\cdot|\theta)$, then the set $\{\theta | x_{\theta} = x\}$ is sampled from $\pi(\theta|x)$. Such an approach would never work in practice as the proportion of $x_{\theta}$ samples equal to $x$ for models with continuous support is zero. For this reason, a dissimilarity $d$ between $x$ and $x_{\theta}$ is used within a decision rule to accept or reject $\theta$. In other words, we sample the set $\{\theta|d(x, x_{\theta}) \leq \epsilon \}$ for some fixed threshold $\epsilon > 0$. Dissimilarity, as defined by @jousselme2012distances, is a weaker notion than distance---without the triangle inequality and where $x{=}y \implies d(x, y){=}0$ is true but the converse $d(x, y){=}0 \implies x{=}y$ is not true in general. 

The FR metric (Equation \@ref(eq:FR)) is a dissimilarity, another dissimilarity which we use is the estimator for maximum mean discrepancy ($d_{\text{MMD}}$), developed by @gretton_kernel_2007 as a non-parametric two-sample statistic for testing whether samples come from the same distribution. @park2016k2 use $d_{\text{MMD}}$ as a dissimilarity within an ABC algorithm. In our case, inputs to dissimilarities are functional data; which we emphasis by denoting them as $f$ and $g$. The definition of $d_{\text{MMD}}(f,g)$ is as follows:
\begin{align}
d_{\text{MMD}}(f,g) &=  \frac{1}{m^2} \sum_{j=1}^m \sum_{j^{\prime} = 1}^{m} k(f_j, f_{j^{\prime}}) + \nonumber \\
&\quad \quad \frac{1}{n^2} \sum_{j=1}^n \sum_{j^{\prime}  = 1 }^n k(g_j, g_{j^{\prime}}) \nonumber \nonumber \\
&\quad \quad - \frac{2}{mn} \sum_{j=1}^m \sum_{j^{\prime}  = 1 }^n  k(f_j, g_{j^{\prime}}), (\#eq:MMD)
\end{align}
where $m$ is the cardinality of $f$, $n$ is the cardinality of $g$ and $k$ is a kernel function. A common choice of kernel function is the Gaussian kernel, $k(f_j,g_{j^{\prime}}) = \exp \left[ -0.5 \sqrt{(t^f_j-t^g_{j^{\prime}})^T S^{-1} (y^f_j-y^g_{j^{\prime}})} \right]$, where $S$ is a fixed tuning covariance matrix. @gretton_kernel_2012 showed that $d_{\text{MMD}}$ is equivalent to a kernel-smoothed L2 norm between EFRVs. We can, therefore, use $\hat{\rho}_{\text{MMD}}$ as a dissimilarity on EFRVs rather than probability measures.

In summary, ABC samplers return samples of $\theta$ depending on the value of $d(f, f_{\theta})$. There exist many varieties of ABC samplers in the literature including: accept-reject, replenishment [@drovandi_estimation_2011]; simulated annealing [@albert_simulated_2015]. We find it useful in this paper to introduce an additional abstraction called a dissimilarity sampler (d-sampler) (Algorithm \@ref(alg:loss)), which helps to simplify the clarify the language used later. 

\begin{algorithm}
\caption{d-sampler}
\label{alg:loss}
\begin{algorithmic}[1]
\Function{}{$\theta, f | p, d$}
\State $f_{\theta} \sim p(\cdot|\theta)$
\State $d_{\theta} = d(f, f_{\theta}) $
\State \Return $d_{\theta}$
\EndFunction
\end{algorithmic}
\end{algorithm}

This notion makes ABC samplers easier to describe. For instance, the rejection sampler can be written as a for loop over $i$ with $\left( \theta_i \sim \pi(\cdot), d_i = \text{d-sampler}(\theta_i, f) \right)$ keeping only pairs with $d_i$ less than $\epsilon$. We adopt the ABC sampler as defined by Drovandi and Pettitt termed the replenishment ABC sampler (Appendix). 

To develop an ABC sampler for functional data, we develop d-samplers on $\theta$ and $f$ based on the warping $\gamma$ of @srivastava2011registration, combined with a discrepancy (either FR \@ref(eq:FR) or MMD \@ref(eq:MMD)) as is shown in Algorithm \@ref(alg:regloss). In the next section we compare these loss functions against their unregistered versions (Algorithm \@ref(alg:loss)). 

\begin{algorithm}
\caption{Registered d-sampler}
\label{alg:regloss}
\begin{algorithmic}[1]
\Function{}{$\theta, f | p, d$}
\State $f_{\theta} \sim p(\cdot|\theta)$
\State $\gamma = \arg \inf_{\gamma \in \Gamma} d_{\text{FR}}(f, f_{\theta} \circ \gamma)$
\State $d_{\theta} = d(f, f_{\theta} \circ \gamma)$
\State \Return $d_{\theta}$
\EndFunction
\end{algorithmic}
\end{algorithm}

# Gaussian peak shift \label{sec:sG}

## Method

We first construct an artificial problem to demonstrate the method we suggest. The mean function $\mu(t)$ is a sum over 14 Gaussian functions, i.e. $\mu(t) = \sum_{u=1}^{14} \phi(t | \mu_u, \sigma_{\phi})$. The observed functional output corresponding to fixed sampling locations $t = (0, 0.5, \cdots, 300)$, are independent and normally distributed $y_j \sim N(\mu(t_j), \sigma_{\epsilon})$. What makes the problem of interest here is that the mean parameters $\mu_u$ are latent random variables $\mu_u = \alpha_u + b_u$, where $b_u \sim N(0, \sigma_b)$ with known values for $\sigma_b$ and each $\alpha_u$. The fact that $b_u$ is random and high dimensional makes inelastic dissimilarities inappropriate. 

The prior distributions for the parameters of interest are $\sigma_{\phi} \sim U(0, 10)$, and $\sigma_{\epsilon} \sim U(0, 0.1)$. An example of a model realisation from this distribution is shown in Figure \@ref(fig:sGExample).

```{r sGExample, fig.cap="Example of model realisations from the Gaussian peak shift model with $\\sigma_b = 5, \\sigma_{\\phi} = 1$, and $\\sigma_{\\epsilon} = 0.01$. The dotted lines are the $\\mu_u$ values. The distribution of peak shifts is controlled by $\\sigma_b$, the peak widths are controlled by $\\sigma_{\\phi }$, and the noise is controlled by $\\sigma_{\\epsilon}$."}
set.seed(1)

t <- seq(0, 200, by = 0.5)
alpha = seq(20, 180, by = 20)
theta = c(1, 0.8, 0.01)

alpha_norm   <- alpha[seq(1,length(alpha), by = 2)]
alpha_cauchy <- alpha[seq(2,length(alpha), by = 2)]


y <- simulator_sGaussian(t, param = theta, 
  alpha_norm = alpha_norm, alpha_cauchy = alpha_cauchy)
z <- simulator_sGaussian(t, param = theta, 
  alpha_norm = alpha_norm, alpha_cauchy = alpha_cauchy)

plot(y, type = "l", lwd = 1, xaxs = "i", yaxs = "i", col = "blue")
lines(z, col = "red")
abline(v = alpha, lty = 2)
```

To test the effectiveness of registered loss functions in this situation, we compare all four combinations, i.e. MMD and FR with and without registration.  We set the covariance matrix for MMD to be the $2 \times 2$ diagonal matrix with elements 9 and $1 \times 10^{-4}$. We use the R package fdasrvf [@tuckerCRAN] to align the simulated and observed functional datasets. To compute the elastic distance between functions as defined by @srivastava2011registration we use the implementation contained in the R package fdasrvf [@tuckerCRAN]. 

## Results

Density plots of posterior samples arising from the ABC sampler with all four loss functions, registered and unregistered, (Figure \@ref(fig:sGaussianPost)) show that registered loss functions outperform their unregistered counterparts. Interestingly the distance used for registration, FR, when used as the dissimilarity for ABC is outperformed by MMD which leads us to believe that the best dissimilarity for registration and the best dissimilarity for ABC do not always coincide. The reason MMD outperforms FR for $\sigma_{\epsilon}$, in particular, is its penalty term. The first term in Equation penalises the concentration of $y^{f_{\theta}}$ such that the algorithm is lead to a higher value of $\sigma_{\epsilon}$ to match the noise in $y^{f}$. We are interested in the effect of registration rather than the dissimilarity, so in further examples, we proceed only with MMD. 

We demonstrate now a more complex example with a dynamic queueing network model of an international airport terminal. 

```{r, sGaussianPost, fig.asp = 0.5, fig.cap="Density plots of posterior samples arising from the replenishment ABC sampler [@drovandi_estimation_2011] for the Gaussian peak shift example. Distances shown include MMD (Maximum mean discrepancy) and FR (Fisher-Rao) in on both registered and unregistered data. Vertical black solid lines represent true values. "}
load("../runs/sGaussian/11/ABC_sG.RData")

output_tidy <- tidyr::gather(ABC_sG, "parameter", , -registration, -sigma_a, -distance)

# output_tidy$parameter <- factor(output_tidy$parameter, labels = c("rho[phi]", "sigma[epsilon]", "sigma[phi]"))

true_params = c(1, 0.01, 0.7)

vline_df = data.frame(parameter = c("sigma[phi]", "sigma[epsilon]","rho[phi]"), input = true_params, value = c(1, 1, 1))

blank_df <- data.frame(parameter = c("sigma[phi]", "sigma[epsilon]","rho[phi]"), value = c(3,0.02,2))

output_tidy <- output_tidy %>%
  mutate(distance = factor(distance)) %>%
  mutate(distance = factor(distance, levels(distance)[c(2,1)])) %>%
  mutate(registration = factor(ifelse(registration, "Registered", "Unregistered"))) %>%
  mutate(registration = factor(registration, levels(registration)[c(2,1)]))

output_tidy$parameter <- factor(output_tidy$parameter, levels = c("sigma[phi]", "rho[phi]", "sigma[epsilon]"))

#blank_df <- data.frame(parameter = rep(levels(output_tidy$parameter), each = 2), value = c(0, 0.02, 0, 3), registration = rep(output_tidy$registration[1], 4), distance = rep(output_tidy$distance[1], 4))

ggplot(output_tidy) +
  aes(x = value, col = registration, linetype = distance) +
  stat_density(position = "identity", geom = "line", adjust = 2) +
  facet_wrap(~parameter, scales = "free", labeller = label_parsed) +
  geom_vline(data = vline_df , mapping = aes(xintercept = input), col = true_color, lty = 1) +
  labs(colour = "Registration status") +
  scale_y_continuous(expand = c(0.02, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, NA)) +
  ggthemes::theme_few() +
  ylab(latex2exp::TeX('$\\pi_{ABC} (\\theta | y)$')) +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), panel.spacing.x = unit(2, "lines"), legend.title = element_blank(), axis.title.x = element_blank() , legend.position = "bottom" , plot.margin = unit(c(0, 2, 0, 0),  "lines")) +
  guides(
    color = guide_legend(order = 0),
    lty = guide_legend(order = 1)
  ) + 
  geom_blank(data = blank_df, mapping = aes(x = value), inherit.aes = FALSE) +
  color_scale
```

# Passenger processing at an international airport \label{sec:Airport}

Rising demand for air travel and enhanced security screening places pressure on existing airport infrastructure. Airport terminal infrastructure, for financial and geographical reasons, is difficult to upgrade at such a rate as to match this demand. With this in mind, operational planners at airports seek to optimise day-to-day operations. However, any optimisation framework requires a somewhat realistic model. To be a realistic model is will necessarily become complicated and this combined with the data collection scheme leads inevitably to intractable likelihoods. On the other hand, simulators are straight-forward to construct, although computationally demanding has prevented simulation-based approaches in the past.

A faster algorithm for queueing based algorithms called QDC [@ebert_computationally_2017] was used by @ebert2018likelihood to construct an ABC sampler for this problem. However, there were problems with misalignment as noted in the discussion of this paper. We consider a simplified (for the sake of exposition) version of the model and data as @ebert2018likelihood. The purpose of this model is to predict passenger flows through an airport terminal in response to particular flight schedules and staff rosters. The data comprise records of passenger numbers passing through certain check-points for each minute of the day. The flight schedule and staff rosters can be thought of as explanatory variables since they are known and affect the response variable (passenger flows) to some degree which we wish to infer. 

## Method

We model the arrivals terminal of an international airport. For one day we have a list of flights $u$ along with their arrival gate $g_u$ and arrival time $a_u$. The walking distance $m_u$ from gate $g_u$ to immigration is also known. To model passenger flows, we simulate every passenger from every flight. The time at which passenger $v$ from flight $u$ deplanes is $a_{u,v}^{\text{dpl}} = a_{u} + t_{u,v}^{\text{dpl}} + b_u$ where $t_{u,v}^{\text{dpl}}$ is the time taken for the passenger to leave the aircraft and $b_u$ is an unobserved amount of time (0 - 20min) taken by the crew after the recorded arrival time to open the doors, letting passengers out. Note that in this example $b_u$ plays the same role as the peak shift example, that of a high dimensional latent variable shifting peaks independently.  

Once passengers have deplaned, they walk to the immigration system and queue for processing. The times at which customers arrive to the queueing system are $a_{u,v}^{\text{imm}} = a_{u,v}^{\text{dpl}} + t_{u,v}^{\text{imm}}$ where $t_{u,v}^{\text{imm}}$ is the walking time to immigration from the arrival gate. The distribution of $t_{u,v}^{\text{imm}} | m_u$ is $\text{Gamma} ( \alpha,  \beta/m_p )$. There are two parallel queueing systems, one for local and one for foreign passengers. The proportion of local passengers on each flight is known $l_u$, and encoded in the boolean variable $\text{nat}_{u,v} \sim \text{Bern}(l_u)$. Each queueing system proceeds at a different rate $\lambda_{\text{nat}}$ and has a different number of servers which are non-interchangeable, which is an accurate depiction of real systems including staff and automated systems working in parallel. 

We require a queueing simulator to compute the times at which passengers leave their queueing system. A queueing simulator (QueueSim) is a function which (conditional on arrival times, service times and number of servers) deterministically computes times at which passengers leave their queueing system (departure times). First, we partition the arrival times $a_{u,v}^{\text{imm}}$ by $\text{nat}_{u,v}$; then we sample service times $s_{u,v} \sim \text{Exp}(\lambda_{\text{nat}_{u,v}})$. The number of servers $K_{\text{nat}}$ are known. The departure times from the queueing systems are $\mathbf{z}^{\text{imm}}_{\text{nat}} = \text{QueueSim}(\mathbf{a}^{\text{imm}}_{\text{nat}}, \mathbf{s}_{\text{nat}},   K_{\text{nat}})$, the variables are written in boldface to denote the full vectors partitioned by nationality since the movements of each passenger, once queueing begins, are no longer independent. We bin $\mathbf{a}_{\theta}^{\text{imm}}$ and $\mathbf{z}^{\text{imm}}$ by minute to construct $f^{a}_{\theta}$ and $f^{z}_{\theta}$ respectively. The synthetic data were also constructed in this manner so that we could know the true parameter values. 

These two functional variables $f^{a}_{\theta}$ and $f^{z}_{\theta}$ represent input and output of the immigration system respectively. We cannot use input data $f^{a}$ directly within the model to produce realisations $f^{z}_{\theta} | f^{a}, \theta$ since the transformation from $\mathbf{a}_{\theta}^{\text{imm}}$ is non-invertible, and we do not know the flight number for observed passengers. We can use curve registration to find the warping function $\gamma$ from $f^{a}_{\theta}$ to $f^{a}$ and apply this warping (warp) to the latent variable $\mathbf{a}^{\text{imm}}$ so as to produce "corrected" $\mathbf{z}^{\text{imm}}$ and ultimately corrected $f^{z}_{\theta}$ (Algorithm \@ref(alg:AirportLoss)). The type argument encodes whether the d-sampler is unregistered, registered or corrected. For the registered d-sampler, only the dissimilarity on $f^{a}_{\theta}$ is registered, the corrected d-sampler uses a registered dissimilarity on $f^{a}_{\theta}$ and uses the $\gamma$ to correct the input for a more accurate output sample $\mathbf{z}^{\text{imm}}$. 

\begin{algorithm}
\caption{Airport d-sampler}
\label{alg:AirportLoss}
\begin{algorithmic}[1]
\Function{}{$\theta, f^a, f^z, \text{type}$}
\State $\mathbf{a}^{\text{imm}}_{\theta} \sim p_a(\cdot | \mu, \nu)$
\State $f^a_{\theta} = \text{hist}(\mathbf{a}^{\text{imm}}_{\theta})$
\State $\breve{\mathbf{a}}^{\text{imm}}_{\theta} = \mathbf{a}^{\text{imm}}_{\theta}$
\If{type = registered or corrected}
  \State $\gamma = \arg \inf_{\gamma \in \Gamma} d_{\text{FR}}(f^a, f^a_{\theta} \circ \gamma)$ 
  \State $d^a_{\theta} = d(f^a, f^a_{\theta} \circ \gamma)$
  \If{type = corrected}
    \State $\breve{\mathbf{a}}^{\text{imm}}_{\theta} = \text{warp}(\mathbf{a}^{\text{imm}}_{\theta}, \gamma)$
  \EndIf
\Else
  \State $d^a_{\theta} = d(f^a, f^a_{\theta})$
\EndIf
\State $\mathbf{z}^{\text{imm}}_{\theta} \sim p_a(\cdot |\breve{\mathbf{a}}^{\text{imm}}_{\theta}, \lambda, K)$
\State $f^z_{\theta} = \text{hist}(\mathbf{z}^{\text{imm}}_{\theta})$
\State $d_{\theta} = d^a_{\theta} + d(f^z, f^z_{\theta})$
\State \Return $d_{\theta}$
\EndFunction
\end{algorithmic}
\end{algorithm}

## Results

Density plots of posterior samples for all parameters of interest for each d-sampler type are shown in Figure \@ref(fig:AirportPost). We see that the registered d-sampler outperforms the unregistered loss for $\mu$ and $\nu$ which concern walking speed. Furthermore, we see that applying the correction to the input to the immigration system improves the accuracy of the output since the posterior density for the corrected loss outperforms all others for all parameters. The accuracy of output predictions from input observations involving a lossy transformation from unobserved latent variables may benefit from this technique. 

```{r, AirportPost, message=FALSE, fig.cap = "Density plots of posterior samples using the airport loss function (Algorithm 3) for unregistered, registered and corrected types. The solid vertical black line represents the true value."}
load(file = "../runs/airport/2/ABC_airport.RData")

# data file ABC_airport

param_names <- c("mu", "nu", "lambda[f]", "lambda[l]")
true_params <- c(0.02, 0.64, 0.4, 0.5)

vline_df = data.frame(parameter = param_names, input = true_params, value = rep(1, 4))

airport_tidy <- ABC_airport %>% tidyr::gather("parameter", , -registration, -distance, -correction) %>%
  filter(!(!registration & correction)) %>%
  mutate(out = factor(ifelse(!registration, "Unregistered", ifelse(!correction, "Registered", "Corrected")))) %>%
  mutate(out = factor(out, levels(out)[c(3,2,1)]))

# %>% mutate(distance = paste0(distance, ifelse(registration, "", " (reg)")))

airport_tidy$parameter <- factor(airport_tidy$parameter, labels = c("lambda[f]", "lambda[l]", "mu", "nu"))

blank_df <- data.frame(parameter = rep(levels(airport_tidy$parameter), each = 2), value = c(0, 1, 0, 1, 0, 0.05, 0, 1), out = rep(airport_tidy$out[1], 4))

ggplot(airport_tidy %>% filter(distance == "MMD")) +
  aes(x = value, col = out) +
  stat_density(position = "identity", geom = "line") +
  facet_wrap(~parameter, scales = "free", labeller = label_parsed) + 
  scale_y_continuous(expand = c(0.02, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, NA)) +
  geom_vline(data = vline_df , mapping = aes(xintercept = input), col = true_color, linetype = 1) +
  ggthemes::theme_few() +
  ylab(latex2exp::TeX('$\\pi_{ABC} (\\theta | y)$')) +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), panel.spacing.x = unit(2, "lines"), legend.title = element_blank(), axis.title.x = element_blank() , legend.position = "bottom" , plot.margin = unit(c(0, 2, 0, 0),  "lines")) +
  geom_blank(data = blank_df) +
  color_scale
```

# Hydrological modelling \label{sec:Hydro}

Water flows within catchment areas, in response to rainfall and other weather events, are forecasted with run-off models. Runoff models have been used successfully by .... in particular, the GR4J (Génie Rural à 4 paramètres Journalier) model of @perrin2003improvement is widely used. Water flows in hydrology are represented by hydrographs representing volumetric flow-rates at a particular point (see Figure \@ref(fig:HydroExample)).

```{r HydroExample, fig.cap="Synthetic hydrograph with associated rain and evaporation data from the airGR package", message = FALSE, warning=FALSE}
library(airGR)
library(ggplot2)
data(L0123001)

InputsModel <-
  CreateInputsModel(
    FUN_MOD = RunModel_GR4J,
    DatesR = BasinObs$DatesR,
    Precip = BasinObs$P,
    PotEvap = BasinObs$E
  )

Ind_Run <-
  seq(which(format(BasinObs$DatesR, format = "%d/%m/%Y") == "01/03/1997"),
      which(format(BasinObs$DatesR, format = "%d/%m/%Y") == "01/01/1998"))

RunOptions <- CreateRunOptions(
  FUN_MOD = RunModel_GR4J,
  InputsModel = InputsModel,
  IndPeriod_Run = Ind_Run,
  IniStates = NULL,
  IniResLevels = NULL,
  IndPeriod_WarmUp = NULL
)

Param <- c(257, 1, 88, 2.2, 0.05)

OutputsModel <-
  RunModel_GR4J(InputsModel = InputsModel,
                RunOptions = RunOptions,
                Param = Param[1:4])

#plot(OutputsModel, Qobs = BasinObs$Qmm[Ind_Run], which = c("Precip", "Flows"))

Hydrodata <- BasinObs[Ind_Run,]

ggplot(Hydrodata) +
  geom_line(aes(y = P/4, group = "Rainfall", col = E), size = 0.5) +
  aes(x = DatesR, y = Qmm, group = "Flow rate", linetype = "Flow rate") +
  geom_line(col = "black") +
  ggthemes::theme_few() +
  scale_x_datetime(breaks = scales::pretty_breaks(12), date_labels = "%b") +
  scale_y_continuous(sec.axis = sec_axis(~.*4, name = "Rainfall (mm/day)")) +
  labs(y = "Flow rate (mm/day)") +
  theme(legend.position = "top", axis.title.x = element_blank()) + scale_color_gradient2(midpoint = 2, low = "blue", high = "red", mid = "grey") + guides(col = guide_colourbar(title = "Evaporation"), linetype = guide_legend(title = element_blank()))
```

Parameter estimation proceeds with either expert opinion or automatic calibration. Automatic calibration is typically performed with a goodness-of-fit function such as sum of squared errors rather than a likelihood function. Parameter estimation for hydrological methods is an active area of research [@kavetski2018parameter]. @mcinerney2018simplified argues that once parameters are estimated the parameter uncertainty itself contributes little to overall uncertainty compared to the residual error structure. We apply approximate Bayesian computation to this application for the first time and apply the methodology of curve-registration. 

## Methods

The simulation model we use is the GR4J model of @perrin2003improvement. This model contains four parameters labelled: the first three $(\theta_1, \theta_2, \theta_3)$ are capacities in units of length (mm); and the fourth parameter ($\theta_4$) represents a lagged effect in units of time (days). Predictions from the GR4J, given parameters $\theta = (\theta_1, \theta_2, \theta_3, \theta_4)$ and input observations $f^a$, are denoted as $\mathcal{H}(\theta, f^a)$. Like many run-off models, the GR4J model is deterministic. To provide useful prediction intervals, hydrologists typically add an error structure on top of the deterministic model. Run-off predictions are positively valued, and it is well known that errors are heteroscedastic [@kavetski2018parameter]. A common strategy is to perform a Box-Cox transformation $B(x, \lambda) = (x^{\lambda} - 1) / \lambda$ to deterministic output, add noise $\epsilon$ and then invert the Box-Cox transformation: 
\begin{align}
	f^z &= B^{-1}[B[\mathcal{H}(\theta, f^a)] + \epsilon], \\
	\epsilon &\sim N(0, \sigma^2).
\end{align}
The model is an input/output model like the airport example, however, in this case, the input observation $f^a$ and the latent input variable are one and the same, so we do not include a corrected loss. We use the same loss functions as were used in the peak shifted example (Algorithms \@ref(alg:loss) and \@ref(alg:regloss)). 

## Results

Although this example includes peak shifting as is found in the previous two examples we see from density plots of posterior samples (Figure \@ref(fig:HydroPost)) that the unregistered loss outperforms the unregistered loss. In those cases, there was a high dimensional latent variable $b_u$ affecting peak shift location, in this case, there is only one variable $\theta_4$. The process of registration is here unnecessary and adds noise to the loss function flattening the posterior densities.  

```{r, HydroPost, warning = FALSE, fig.cap = "Density plots of posterior samples from the hydrological model using unregistered and registered loss function."}
load("../runs/hydrology/3/ABC_hydro_syn.RData")

true_params <- c(257, 1, 88, 10, 0.05)
param_names <- names(ABC_hydro_syn)[1:5]

hydro_syn_tidy <- ABC_hydro_syn %>%
  tidyr::gather("parameter", ,-data, -registration, -distance) %>%
  mutate(registration = factor(ifelse(registration, "Registered", "Unregistered"))) %>%
  mutate(registration = factor(registration, levels(registration)[c(2,1)]))

hydro_syn_tidy$parameter <- factor(hydro_syn_tidy$parameter, labels = c("sigma", "theta[1]", "theta[2]", "theta[3]", "theta[4]"))

vline_df = data.frame(parameter = param_names, input = true_params, value = rep(1, 5))

vline_df$parameter <- factor(vline_df$parameter, labels = c("sigma", "theta[1]", "theta[2]", "theta[3]", "theta[4]"))

blank_df <- data.frame(parameter = rep(levels(hydro_syn_tidy$parameter), each = 2), value = c(0.01, 0.08, 100, 1200, -5, 3, 20, 3, 1.1, 30), registration = rep(hydro_syn_tidy$registration[1], 10))

ggplot(hydro_syn_tidy %>% filter(distance == "MMD")) +
  aes(x = value, col = registration) +
  stat_density(position = "identity", geom = "line") +
  facet_wrap(~parameter, scales = "free", labeller = label_parsed) + 
  scale_y_continuous(expand = c(0.02, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0), limits = c(NA, NA)) +
  geom_vline(data = vline_df , mapping = aes(xintercept = input), col = true_color, linetype = 1) +
  ggthemes::theme_few() +
  ylab(latex2exp::TeX('$\\pi_{ABC} (\\theta | y)$')) +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), panel.spacing.x = unit(2, "lines"), legend.title = element_blank(), axis.title.x = element_blank() , legend.position = "bottom" , plot.margin = unit(c(0, 2, 0, 0),  "lines")) +
  geom_blank(data = blank_df) +
  color_scale
```

# Discussion

We have demonstrated two ways through which curve registration can be used in aid of likelihood-free inference for functional data. In the peak shifts example of Section \@ref(sec:sG) we saw how registered loss based on both FR and MMD dissimilarities outperform their unregistered types. Another use of the curve registration is shown in the airport example of Section \@ref(sec:Airport) where curve registration was used to convert input data to produce more realistic output data. Finally, in the hydrological example, the ABC sampler correctly identifies appropriate values for $\theta_4$ making unregistered loss functions unnecessary.

* Mention alternative of estimating $b_u$ and how we avoid this problem with elastic distances. 

# References


