---
title: "Registration of functional datasets for likelihood-free inference"
author: "Anthony Ebert, Paul Wu, Kerrie Mengersen, Fabrizio Ruggeri"
date: "16 October 2018"
output: 
  html_document:
    self_contained: false
bibliography: ../Zotero_link.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(CurveRegistration)
library(dplyr)
library(ggplot2)
```

\newcommand{\fsf}{\mathsf{f}}
\newcommand{\gsf}{\mathsf{g}}
\newcommand{\E}{\mathsf{E}}

# Abstract

Likelihood-free inference on functional data requires the specification of sensible distance measures on these functions. We consider the problem of functional datasets which display not only amplitude variation but also phase variation. We show how current methods in curve registration can be applied to approximate Bayesian computation with the help of kernel-based methods recently developed for metrics for probability measures.  

# Introduction

Functional data analysis (FDA) address statistical problems involving a functional dataset, where the data comprise a set of functions [@hsing2015theoretical]. A functional random variable $\mathsf{y}$ is a random variable which takes values in a function space $\mathsf{f} \in \mathcal{F}$ with associated probability measure $p$. We consider function spaces of the form $\mathbb{R}^{[0, 1]}$, that is $\mathsf{f}: [0, 1] \mapsto \mathbb{R}$, transformations from different domain intervals is straight-forward. A functional dataset is a collection of observed functional random variables $\mathsf{f}_1, \cdots, \mathsf{f}_n$. In practice the functional random variables are observed with error on a countable subset of the domain of the function and this is what is referred to as a functional dataset, which contains empirical functional random variables. An empirical functional random variable (EFRV) $f_i$ is of the form $\{ (t_{i,1},y_{i,1}), \cdots , (t_{i,j}, y_{i,j}), \cdots, (t_{i,n_i}, y_{i,n_i}) \}$, for fixed sampling points $t_i = \{t_{i,1}, \cdots, t_{i,n_i} \}$ and corresponding observed functional outputs $y_i = y_{i,1}, \cdots, y_{i,n_i}$. 

Common special-cases of functional datasets include longitudinal data and time series, however the field of FDA is much broader than this [@wang2016functional]. In analysis of functional data, a statistic of interest is an estimate of the mean function $\mu(t) = \underset{\fsf \sim p}{\E}[\mathsf{f}](t)$. In the case of EFRVs with fixed sampling locations ($t_1 = \cdots = t_i = \cdots = t_{n_i}$) this is usually estimated with $\hat{\mu}(t_{,j}) = 1/n_i \sum_{i = 1}^{n_i} y_j$. In either case it is possible that the resulting function is not an element of $\mathcal{F}$ due to phase variation. For instance suppose $\mathcal{F}$ is the set of Gaussian functions $\phi(\mu,\sigma)$ with $\mu \sim U(0, 5)$, $\sigma = 1$. The mean function, in this case, is not Gaussian.  In the next section we learn how problems of this sort are addressed in literature. 

There are a variety of approaches in the literature which seek to address the provide statistical tools in the presence of phase variation, these include: dynamic time warping [@Padoy2012632,@wang1997alignment]; the Frechet distance [@rote2007computing]; curve registration and elastic functions [@srivastava2011registration]. Applications of curve registration include: growth curves [@cheng2016bayesian]; surgical workflow [@Padoy2012632]. 

We standardise the domain of $\mathsf{f^*} \in \mathbb{R}^{[0, T]}$ from $[0,T]$ to $[0,1]$ by setting $\mathsf{f}(t) = \mathsf{f^*}(t \times T)$. The idea of curve registration is to align elements of $\mathcal{F}$ with warping functions $\gamma_i: [0,1] \mapsto [0,1]$, such that elements of the set $\mathcal{G} := \{\mathsf{f}_i \circ \gamma_i | \mathsf{f}_i \in \mathcal{F} \}$ have features which are aligned. We adopt the elastic functions approach of @srivastava2011registration, their approach is to find $\gamma$ which minimises the Fisher-Rao metric. The distance used for this approach is the Fisher-Rao metric: 
$$\rho_{\text{FR}}(\fsf,\gsf) = \int [ q_{\fsf}(t) - q_{\gsf}(t) ]^2 \text{d} t , $$
where $q_f(t) = \text{sign}(f^{\prime}(t)) \times \sqrt{|f^{\prime}(t)|}$. We align a curve $\fsf$ to $\gsf$ by defining a warping function $\gamma \in \Gamma$ where $\Gamma = \{\gamma \in [0,1]^{[0, 1]} | \gamma \text{ is invertible and } \gamma(0) = 0 \}$ to minimise $\rho_{\text{FR}}(\fsf \circ \gamma, \tilde{\gsf})$. The advantage of using $\rho_{\text{FR}}$ for curve registration is that:
$$\rho_{\text{FR}}(\fsf, \gsf) = \rho_{\text{FR}}(\fsf \circ \gamma, \gsf \circ \gamma) $$
in otherwords the Fisher-Rao metric is invariant to a shared warping. This implies that the discrepancy of amplitudes (amplitude distance) between $\fsf$ and $\gsf$, defined as
$$\rho_{\text{amp}}(\fsf,\gsf) := \inf_{\gamma \in \Gamma} \rho_{\text{FR}}(\fsf \circ \gamma, \gsf)$$ is symmetric [@srivastava2011registration]. In practice EFRVs $f$ are used in place of true unobserved functions $\fsf$ and the same approach proceeds with straight-forward approximations.

For simplicity and without loss of generality we consider problems where the functional dataset consists of a single observed EFRV $f$. We extend curve registration to the likelihood-free domain with approximate Bayesian computation (ABC). We proceed now with a brief background of ABC, for a detailed exposition on the subject see @sisson2018handbook.

# Approximate Bayesian Computation

ABC samplers sample $\theta$ from the posterior distribution $\pi(\theta|\mathbf{x})$ by sampling from, but not evaluating, the model $p(\mathbf{x}|\theta)$. This is useful where $p(\mathbf{x}|\theta)$ represents a complicated simulator for generating realisations $\mathbf{x}_{\theta} \sim p(\cdot|\theta)$. 

The theoretical foundation of ABC rests on the fact that if we draw samples $\theta$ independently from the prior $\pi(\theta)$ and use each $\theta$ to generate a corresponding $\mathbf{x}_{\theta}$ then the set $\{\theta | \mathbf{x}_{\theta} = \mathbf{x}\}$ are sampled from $\pi(\theta|\mathbf{x})$. Such an approach would never work in practice as the proportion of $\mathbf{x}_{\theta}$ samples equal to $\mathbf{x}$ for models with continuous support is zero. For this reason a dissimilarity $\rho$ defined on the sample space is used to accept or weight realisations $\mathbf{x}_{\theta}$ in relation to $\mathbf{x}$ according to some threshold $\epsilon>0$. 

Dissimilarity, as defined by @jousselme2012distances, is a weaker notion than distance without the triangle inequality and where $\mathbf{x}{=}\mathbf{y} \implies \rho(\mathbf{x}, \mathbf{y}){=}0$ is true but the converse $\rho(\mathbf{x}, \mathbf{y}){=}0 \implies \mathbf{x}{=}\mathbf{y}$ is not necessarily true. For instance a common approach is to define $\rho$ as a distance on lower dimensional summary statistics, this is equivalent to a dissimilarity on the sample space. 

We adopt the ABC sampler as defined by @drovandi_estimation_2011 termed the replenishment ABC sampler. 

To use an ABC sampler with functional data we develop an elastic dissimilarity defined on $\mathcal{F}$, based on the curve-registration approach of @srivastava2011registration combined with a discrepancy between probability distribution called maximum mean discrepancy. 

Maximum mean discrepancy (MMD), as introduced by @gretton_optimal_2012, is a metric between probability measures $p, q$ on a common probability space $\mathcal{Q}$:

\begin{align}
\rho_{\text{MMD}}(p, q, \mathcal{H}) = \sup_{h \in \mathcal{H}} \left( \int h(x) [p(x) - q(x)] \text{d} x \right),
\end{align}

where $\mathcal{H}$ is a function space defined on the same domain as the probability space $\mathcal{Q}$. The function space is taken by @gretton_kernel_2012 to be set of functions which integrate to 1 on the probability space of interest. @gretton_kernel_2012 also developed an estimator for this metric based on observed datasets $\mathbf{x} \sim p$, $\mathbf{y} \sim q$. The estimator is:

\begin{align}
\hat{\rho}_{\text{MMD}}(\mathbf{x},\mathbf{y}) &=  \frac{1}{m^2} \sum_{i=1}^m \sum_{j = 1}^m k(x_i, x_j) +
\frac{1}{n^2} \sum_{i=1}^n \sum_{j = 1}^n k(y_i, y_j) \label{eq:MMD} 
- \frac{2}{mn} \sum_{i=1}^m \sum_{j = 1}^n  k(x_i, y_j), \nonumber
\end{align}

where $m$ is the length of $\mathbf{x}$, $n$ is the length of $\mathbf{y}$ and $k$ is a kernel function. A common choice of kernel function is the Gaussian kernel $k(\mathbf{x},\mathbf{y}) = \exp \left[ -0.5 \sqrt{(\mathbf{x}-\mathbf{y})^T S_k^{-1} (\mathbf{x}-\mathbf{y})} \right]$, where $S_k$ is a fixed tuning covariance matrix. For univariate and multivariate distributions defining a kernel function between elements of the observed datasets is easy. It has been shown that $\hat{\rho}_{\text{MMD}}$ is equivalent to a kernel-smoothed L2 norm between EFRVs. We can therefore use $\hat{\rho}_{\text{MMD}}$ as a dissimilarity on EFRVs rather than probability measures.

We compare three dissimilarity measures: 'MMD' which is $\rho(f, f_{\theta}) = \hat{\rho}_{\text{MMD}}(f, f_{\theta})$, 'Elastic distance' $\rho(f, f_{\theta}) = \inf_{\gamma \in \Gamma} \rho_{\text{FR}}(\fsf \circ \gamma, \gsf)$ and 'Registered MMD' defined as follows:

\begin{align}
\gamma &= \arg \inf_{\gamma \in \Gamma} \rho_{\text{FR}}(f, f_{\theta} \circ \gamma), \\
\rho(f, f_{\theta}) &:= \hat{\rho}_{\text{MMD}}(f, f_{\theta} \circ \gamma).
\end{align}

In other words we first align $f_{\theta}$ to $f$ and then use $\hat{\rho}_{\text{MMD}}$ as the dissimilarity between aligned functions. Note that Elastic distance uses the same distance as is used to find the optimal warping function under the Registered MMD distance.

# Explanatory example

## Method

We first construct an artificial problem to demonstrate the method we suggest. The model is a series of Gaussian functions $i = 1, \cdots, 14$ added together with fixed/known sampling locations $t = (0, 0.5, \cdots, 300)$. The mean parameter for each $i$ is $\mu_i = \alpha_i + a_i$ where each $\alpha_i$ is a known, fixed effect and $a_i$ are iid unknown random effects. Each Gaussian function has the same unknown standard deviation $\sigma_{\phi}$. The known, fixed effects $\alpha$ are equal to $( 20, 40, \cdots, 280 )$. In addition there is Gaussian noise with standard deviation $\sigma_{\epsilon}$. The unknown parameters of interest are $\sigma_{\phi}$ and $\sigma_{\epsilon}$. 

\begin{align}
    a_i &\sim N(0, \sigma_a) \\
	y_j | \vec{a}, \sigma_a, \sigma_{\epsilon} ; \vec{\alpha}, t_j &\sim N \left\{ \sum_{i=1}^n \phi \left[ \frac{t_j - (a_i + \alpha_i)}{\sigma_{\phi}} \right], \sigma_{\epsilon} \right\}
\end{align}

The prior distributions for each parameter of interest are $\sigma_{\phi} \sim U(0, 10)$, and $\sigma_{\epsilon} \sim U(0, 0.1)$. An example of a model realisation from this distribution is shown in Figure \ref{fig:sGexample}.

```{r sGexample, fig.cap="\\label{fig:sGexample}Example of a draw from the Gaussian peak shift model with $\\sigma_a = 5, \\sigma_{\\phi} = 1$, and $\\sigma_{\\epsilon} = 0.01$. The dotted lines are the $\\alpha$ values. The distribution of peak shifts is controlled by $\\sigma_a$, the peak widths are controlled by $\\sigma_{\\phi }$ and the noise is controlled by $\\sigma_{\\epsilon}$."}
set.seed(1)

t <- seq(0, 300, by = 0.5)
alpha = seq(20, 280, by = 20)
theta = c(1, 0.01, 0, 5)


y <- simulator_sGaussian(t, param = theta, alpha = alpha)

plot(y, type = "l", lwd = 1, xaxs = "i", yaxs = "i", ylim = c(-0.1, 0.5))
abline(v = alpha, lty = 2)
```

To test the effectiveness of the registered MMD method we compare it with the unregistered MMD. We set the covariance matrix for MMD to be the $2 \times 2$ diagonal matrix with elements 9 and $1 \times 10^{-4}$. We use the R package fdasrvf [@tuckerCRAN] to align the simulated and observed functional datsets. To compute the elastic distance between functions as defined by @srivastava2011registration we use the implementation contained in the R package fdasrvf [@tuckerCRAN]. We try two settings for $\sigma_a$ which are 5 and 20.

## Results

The posterior distributions are shown in Figure . As expected the Registered MMD performs better than MMD and this is more obvious when $\sigma_a$ is large. What is surprising is that the Registered MMD performs better than the Elastic distance. @joshi2007novel

```{r}
load("../runs/sGaussian/1/ABC_sG_5.RData")

output_tidy <- tidyr::gather(ABC_sG_5, "parameter", , -registration, -sigma_a, -distance)

# %>%
#   mutate(distance = paste0(distance, ifelse(registration, "", " (reg)")))

output_tidy$parameter <- factor(output_tidy$parameter, labels = c("sigma[epsilon]", "sigma[phi]"))

true_params = c(1, 0.01)

vline_df = data.frame(parameter = c("sigma_phi", "sigma_e"), input = true_params, value = rep(1, 2))

vline_df$parameter <- factor(vline_df$parameter, labels = c("sigma[epsilon]", "sigma[phi]"))

ggplot(output_tidy) +
  aes(x = value, col = distance, linetype = registration) +
  stat_density(position = "identity", geom = "line") +
  facet_wrap(~parameter, scales = "free", labeller = label_parsed) +
  geom_vline(data = vline_df , mapping = aes(xintercept = input), col = "black", lty = 1) +
  labs(colour = "Registration status") +
  scale_y_continuous(expand = c(0.02, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, NA)) +
  ggthemes::theme_few() +
  ylab(latex2exp::TeX('$\\pi_{ABC} (\\theta | y)$')) +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), panel.spacing.x = unit(2, "lines"), legend.title = element_blank(), axis.title.x = element_blank() , legend.position = "bottom" , plot.margin = unit(c(0, 2, 0, 0),  "lines")) +
  guides(
    color = guide_legend(order = 0),
    lty = guide_legend(order = 1)
  )
```

We demonstrate now a more complex example with a dynamic queueing network model of an international airport terminal. 

# Motivating example: Passenger processing at an international airport

Increasing demand for air travel and enhanced security screening places increasing pressure. It is imperative that resources within fixed infrastructure is allocated as efficiently as possible. With this in mind operational planners at airport use simulation models to conduct resource planning, however it is often impossible to perform parameter inference with these models in a principled manner. 

We consider the same model and dataset as @ebert2018likelihood. The data contains functions which record passenger numbers passing through certain check-points in the passenger facilitation process for each minute of the day. We discuss the methodology used to model this dataset. 

To model passenger flows we simulate the movements of each passenger $j$ from each flight $i$. Flight $i$, begin passenger disembarkation at $a_i$, passenger $j$ from this flight disembarks from the aircraft at time $d_{ij}^{\text{dis}} = a_i + t_i^{\text{dis}}$, leaves the arrivals concourse at time $d_{ij}^{\text{ac}} = d_{ij}^{\text{dis}} + t_{ij}^{\text{imm}}$, chooses a route $r_{ij}$ through immigration and finishes immigration at time $d_{ij}^{\text{imm}} = d_{ij}^{\text{ac}} + w_{ij}^{\text{imm}} + s_{ij}^{\text{imm}}$. Distributions for disembarkation times $t_i^{\text{dis}}$ are fitted prior to analysis, however the other random variables are distributed according to unknown parameters $\alpha, \beta, \lambda_{\text{SG}}, \lambda_{\text{MG}}$ in the following manner:

\begin{align}
	&\text{Flight disembarkation start} & a_i &\sim \text{U}(A_i - 20, A_i + 20) \\
	&\text{Passenger disembarkation time} & t_{ij} &\sim \text{G}(\alpha^{\text{dis}}_i, \beta^{\text{dis}}_i) \\
	&\text{Walking times} & t^{\text{ac}}_{ij} &\sim \text{Gamma} \left\{ \alpha,  \frac{\beta}{m_i} \right\},    \\
	&\text{Nationality (local or foreign)} & \text{nat}_{ij} &\sim \text{Bern}(p^{\text{nat}}_{i}),  \\
	&\text{Passenger route (SG or MG)} & r_{ij} | \text{nat}_{ij} &\sim \text{Bern}(p^{\text{imm}}_{\text{nat}_{ij}}), \\
	&\text{Service times} &	s_{ij} | r_{ij} &\sim \text{Exp} \left( \lambda_{r_{ij}} \right),   \\
\end{align}

where $A_i$ is the scheduled time for the flight to begin passenger disembarkation. This is a hierarchical model which consists of flight effects, such as $a_i$ and passenger effects, such as $s_i$. 

The last step to recreate the dataset is to bin the departure vectors $\vec{d}^{\text{ac}}, \vec{d}^{\text{imm}}$, and $\vec{d}^{\text{imm}}_{\text{SG}}$ by minute. We refer to these model realisations as  $\vec{x}^{\text{ac}}, \vec{x}^{\text{imm}}$, and $\vec{x}^{\text{imm}}_{\text{SG}}$ which resemble the observed datasets $\vec{y}^{\text{ac}}, \vec{y}^{\text{imm}}$, and $\vec{y}^{\text{imm}}_{\text{SG}}$. A realisation from this model is shown below. 

We compare three distances: 

\begin{align}
	\textbf{D}_0 &=  \hat{\rho}_{\text{MMD}}(\vec{x}^{\text{ac}},\vec{y}^{\text{ac}}) + \hat{\rho}_{\text{MMD}}(\vec{x}^{\text{imm}},\vec{y}^{\text{imm}}) + \hat{\rho}_{\text{MMD}}(\vec{x}^{\text{imm}}_{\text{SG}},\vec{y}^{\text{imm}}_{\text{SG}}), \\
	\textbf{D}_1 &= \hat{\rho}_{\text{EMM}}(\vec{x}^{\text{ac}},\vec{y}^{\text{ac}}) + \hat{\rho}_{\text{MMD}}(\vec{x}^{\text{imm}},\vec{y}^{\text{imm}}) + \hat{\rho}_{\text{MMD}}(\vec{x}^{\text{imm}}_{\text{SG}},\vec{y}^{\text{imm}}_{\text{SG}}), \\
	\textbf{D}_2 &= \hat{\rho}_{\text{EMM}}(\vec{x}^{\text{ac}},\vec{y}^{\text{ac}}) + \hat{\rho}_{\text{MMD}}(\vec{z}^{\text{imm}},\vec{y}^{\text{imm}}) + \hat{\rho}_{\text{MMD}}(\vec{z}^{\text{imm}}_{\text{SG}},\vec{y}^{\text{imm}}_{\text{SG}}),
\end{align}


## Results

```{r, message=FALSE}
load(file = "../runs/airport/1/ABC_airport.RData")

# data file ABC_airport

param_names <- c("mu", "nu[2]", "lambda[f]", "lambda[l]")
true_params <- c(0.02, 0.64, 0.4, 0.5)

vline_df = data.frame(parameter = param_names, input = true_params, value = rep(1, 4))

airport_tidy <- ABC_airport %>% tidyr::gather("parameter", , -registration, -distance, -correction) 

# %>% mutate(distance = paste0(distance, ifelse(registration, "", " (reg)")))

airport_tidy$parameter <- factor(airport_tidy$parameter, labels = c("lambda[f]", "lambda[l]", "mu", "nu[2]"))

ggplot(airport_tidy %>% filter(correction == FALSE)) +
  aes(x = value, y = stat(count), col = distance, linetype = registration) +
  stat_density(position = "identity", geom = "line") +
  facet_wrap(~parameter, scales = "free", labeller = label_parsed) + 
  scale_y_continuous(expand = c(0.02, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, NA)) +
  geom_vline(data = vline_df , mapping = aes(xintercept = input), col = "black", linetype = 1) +
  ggthemes::theme_few()

ggplot(airport_tidy %>% filter(correction == TRUE)) +
  aes(x = value, y = stat(count), col = distance, linetype = registration) +
  stat_density(position = "identity", geom = "line") +
  facet_wrap(~parameter, scales = "free", labeller = label_parsed) + 
  scale_y_continuous(expand = c(0.02, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, NA)) +
  geom_vline(data = vline_df , mapping = aes(xintercept = input), col = "black", linetype = 1) +
  ggthemes::theme_few()
```

# Counter example: Hydrological modelling

```{r Hydrosetup,include=FALSE}
library(airGR)
library(ggplot2)
data(L0123001)

InputsModel <-
  CreateInputsModel(
    FUN_MOD = RunModel_GR4J,
    DatesR = BasinObs$DatesR,
    Precip = BasinObs$P,
    PotEvap = BasinObs$E
  )

Ind_Run <-
  seq(which(format(BasinObs$DatesR, format = "%d/%m/%Y") == "01/03/1997"),
      which(format(BasinObs$DatesR, format = "%d/%m/%Y") == "01/07/1998"))

RunOptions <- CreateRunOptions(
  FUN_MOD = RunModel_GR4J,
  InputsModel = InputsModel,
  IndPeriod_Run = Ind_Run,
  IniStates = NULL,
  IniResLevels = NULL,
  IndPeriod_WarmUp = NULL
)

Param <- c(257, 1, 88, 2.2, 0.05)

OutputsModel <-
  RunModel_GR4J(InputsModel = InputsModel,
                RunOptions = RunOptions,
                Param = Param[1:4])

plot(OutputsModel, Qobs = BasinObs$Qmm[Ind_Run], which = c("Precip", "Flows"))

Hydrodata <- BasinObs[Ind_Run,]
```


Water flows within catchment areas in response to rainfall and other weather-events are forecasted with run-off models. Runoff models have been used successfully by .... in particular the GR4J (Génie Rural à 4 paramètres Journalier) model of @perrin2003improvement is widely used. 

Water flows in hydrology are represented by hydrographs representing volumetric flow-rates at a particular point (see Figure \ref{fig:Hydroexample}).

```{r Hydroexample, fig.cap="\\label{fig:Hydroexample}Hydrograph with associated rain and evaporation data from airGR package"}

ggplot(Hydrodata) +
  geom_line(aes(y = P/4, group = "Rainfall", col = E), size = 0.5) +
  aes(x = DatesR, y = Qmm, group = "Flow rate", linetype = "Flow rate") +
  geom_line(col = "black") +
  ggthemes::theme_few() +
  scale_x_datetime(breaks = scales::pretty_breaks(12), date_labels = "%b") +
  scale_y_continuous(sec.axis = sec_axis(~.*4, name = "Rainfall (mm/day)")) +
  labs(y = "Flow rate (mm/day)") +
  theme(legend.position = "top", axis.title.x = element_blank()) + scale_color_gradient2(midpoint = 2, low = "blue", high = "red", mid = "grey") + guides(col = guide_colourbar(title = "Evaporation"), linetype = guide_legend(title = element_blank()))
```

Parameter estimation proceeds with either expert opinion or automatic calibration. Automatic calibration is typically performed with a goodness-of-fit function such as sum of squared errors rather than a likelihood function. The NSE function is a goodness-of-fit function specifically developed for hydrology. Parameter estimation for hydrological methods is an active area of research @kavetski2018parameter. It is argued in the literature that once parameters are estimated the parameter uncertainty itself contributes little to overall uncertainty @mcinerney2018simplified compared to the residual error structure.  

We apply approximate Bayesian computation for this application for the first time and apply the methodology of curve-registration. 

## Methods

The simulation model we use is the GR4J model of @perrin2003improvement. This model contains four parameters labelled: the first three $(\theta_1, \theta_2, \theta_3)$ are capacities in units of length (mm); and the fourth parameter ($\theta_4$) represents a lagged effect in units of time (days).

Predictions from the GR4J given parameters $\theta = (\theta_1, \theta_2, \theta_3, \theta_4)$ and explanatory variables (X) are denoted as $\mathcal{H}(\theta, X)$. Like many run-off models the GR4J model is deterministic, the practice is to add an error structure on top of this deterministic model, the model in its entirity is then: 

\begin{align}
	Y = \mathcal{H}(\theta, X) + \epsilon.
\end{align}


# References

Run-off predictions are positively valued, and it is well known that heteroscedasticity exists in the errors [@kavetski2018parameter]. A common strategy is to perform a Box-Cox transformation $Z(Y, \lambda) = (Y^{\lambda} - 1) / \lambda$ to transformed predictions with normalised errors $\eta$:

\begin{align}
	Y_Z = Z(\mathcal{H}(\theta, X)) + \eta.
\end{align}

It has been found that accurate predictions must account of auto-correlation amongst $\eta$, so we use an AR(1) model with parameter $\phi$ and white-noise variance $\sigma^2_W$. We apply the MMD, Registered-MMD and elastic distances as shown previously. The final model is therefore: 

\begin{align}
	Y &= Z^{-1}[Z(\mathcal{H}(\theta, X)) + \eta], \\
	\eta_t &= \phi \eta_{t-1} + W, \\
	W &\sim N(0, \sigma^2_W). 
\end{align}

Once again we test all three distances. 

## Results and Discussion

In this example we find that registered distances perform more poorly than their unregistered counterparts. 

```{r}
load("../runs/hydrology/2/ABC_hydro_syn.RData")

true_params <- c(257, 1, 88, 2.2, 0.05)
param_names <- names(ABC_hydro_syn)[1:5]

hydro_syn_tidy <- ABC_hydro_syn %>%
  tidyr::gather("parameter", ,-data, -registration, -distance) 

# %>%
#   mutate(distance = paste0(distance, ifelse(registration, "", " (reg)")))

hydro_syn_tidy$parameter <- factor(hydro_syn_tidy$parameter, labels = c("sigma", "x[1]", "x[2]", "x[3]", "x[4]"))

vline_df = data.frame(parameter = param_names, input = true_params, value = rep(1, 5))

vline_df$parameter <- factor(vline_df$parameter, labels = c("sigma", "x[1]", "x[2]", "x[3]", "x[4]"))

ggplot(hydro_syn_tidy) +
  aes(x = value, col = distance, linetype = registration) +
  stat_density(position = "identity", geom = "line") +
  scale_y_continuous(expand = c(0.02, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, NA)) +
  facet_wrap(~parameter, scales = "free", labeller = label_parsed)  + 
  geom_vline(data = vline_df , mapping = aes(xintercept = input), col = "black", linetype = 1) + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom")


```


```{r, eval = FALSE}
load("../runs/hydrology/test2/ABC_hydro_obs_RF_MD.RData")
load("../runs/hydrology/test2/ABC_hydro_obs_RT_MD.RData")

sim_data <- simulator_hydro(true_params, InputsModel, RunOptions)

predictions_noreg <- apply(ABC_hydro_obs_RF_MD[,1:5], 1, simulator_hydro, InputsModel = InputsModel, RunOptions = RunOptions, one_d = TRUE)

predictions_reg <- apply(ABC_hydro_obs_RT_MD[,1:5], 1, simulator_hydro, InputsModel = InputsModel, RunOptions = RunOptions, one_d = TRUE)

quantiles_noreg <- apply(predictions_noreg, 1, quantile, probs = c(0.1, 0.5, 0.9)) %>% t()

quantiles_reg <- apply(predictions_reg, 1, quantile, probs = c(0.1, 0.5, 0.9)) %>% t()

day_num <- sim_data[,1]
obs_true <- as.matrix(data.frame(day_num = day_num, value = BasinObs$Qmm[Ind_Run]))

tidy_noreg <- quantiles_noreg %>% as_data_frame() %>% mutate(day_num = day_num) %>% mutate(reg = FALSE)

tidy_reg <- quantiles_reg %>% as_data_frame() %>% mutate(day_num = day_num) %>% mutate(reg = TRUE)


tidy_df <- dplyr::bind_rows(tidy_noreg, tidy_reg)

ggplot(tidy_df) + aes(x = day_num, ymin = `10%`, ymax = `90%`) + geom_ribbon(alpha = 0.5, fill = "red") + facet_wrap(~reg) + geom_line(data = as_data_frame(obs_true), mapping = aes(x = day_num, y = value), inherit.aes = FALSE)

```

# Discussion

We have compared likelihood-free inference for three problems using three distances. The distances were the elastic distance (as proposed by ...), maximum mean discrepancy (Gretton et al...) and a distance incorporating techniques from both registered maximum mean discrepancy. 

The first two problems were amenable to this new distance and the last problem was not. Although all three problems incorporate some aspect of peak position uncertainty, the degrees of freedom for the first two problems was much larger than the last. 

# Information for Anthony

```{r}
writeLines(capture.output(sessionInfo()))
```

# References


